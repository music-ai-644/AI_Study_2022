{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/music-ai-644/AI_Study_2022/blob/main/week8_quiz_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMRHgENqVdcU"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from argparse import ArgumentError\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeZiHPuBVgce"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "KqZcE_KvV7_w",
    "outputId": "14887213-dda0-4f15-b687-47013ffc03eb"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "\n",
    "for i in range(10):\n",
    "    plottable_image = np.reshape(x_train[i], (28, 28))\n",
    "    ax = fig.add_subplot(2, 5, i+1)\n",
    "    ax.imshow(plottable_image, cmap='gray_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data specific\n",
    "input_shape = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "learning_rate = 1e-3\n",
    "NUM_LAYERS = 6 # Input + hidden layer + Output layer\n",
    "HIDDEN_SIZES = [input_shape, 512, 256, 128, 64, 32, num_classes] # PLEASE change only intermediate numbers of hidden sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMax_Scaler(np_train, np_test): \n",
    "    train_min = np.min(np_train)\n",
    "    train_max = np.max(np_train)\n",
    "    \n",
    "    scaled_train = (np_train - train_min) / train_max\n",
    "    scaled_test = (np_test - train_min) / train_max\n",
    "    \n",
    "    return scaled_train, scaled_test\n",
    "    \n",
    "def to_categorical(np_labels): # must be numpy array indicating indices of classes.\n",
    "    '''\n",
    "    One-hot encoding\n",
    "    '''\n",
    "    return np.eye(np.max(np_labels) + 1, dtype='float')[np_labels]\n",
    "\n",
    "# normalize pixel 0 ~ 255 to 0 ~ 1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# minmax scaling \n",
    "x_train, x_test = MinMax_Scaler(x_train, x_test)\n",
    "\n",
    "# one-hot encoding\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# Resize for PyTorch Model\n",
    "x_train, x_test = np.expand_dims(x_train, axis=1), np.expand_dims(x_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "        self.x_data = x\n",
    "        self.y_data = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x_item = self.x_data[index]\n",
    "        y_item = self.y_data[index]\n",
    "        return x_item, y_item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "train_loader = DataLoader(MNISTDataset(x_train, y_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(MNISTDataset(x_test, y_test), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hRrkl0MYjtf"
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_shape=28*28, num_layers=3, hidden_sizes=[3, 3, 3], num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        if num_layers + 1 != len(hidden_sizes):\n",
    "            raise ArgumentError(\"Check number of layers and hidden sizes you want.\")\n",
    "        \n",
    "        # Initialize\n",
    "        self.input_shape = input_shape\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Construct Neural Networks\n",
    "        self.DNN = self._make_layer()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.DNN(x)\n",
    "        return F.log_softmax(output, dim = 1)\n",
    "        \n",
    "    def _make_layer(self):\n",
    "        layers = []\n",
    "        \n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "        for idx in range(0, self.num_layers):\n",
    "            ANN = nn.Sequential(\n",
    "                nn.Linear(self.hidden_sizes[idx], self.hidden_sizes[idx + 1]),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            layers.append(ANN)\n",
    "            \n",
    "        # last layer: No ReLU()\n",
    "        last_ANN = nn.Sequential(\n",
    "                nn.Linear(self.hidden_sizes[self.num_layers], self.num_classes)\n",
    "            )\n",
    "        layers.append(last_ANN)\n",
    "        \n",
    "        return nn.Sequential(*layers) # unpacking list\n",
    "    \n",
    "model = DNN(input_shape = input_shape, num_layers=NUM_LAYERS, hidden_sizes=HIDDEN_SIZES, num_classes=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        x_train, y_train = batch\n",
    "        x_train, y_train = x_train.to(DEVICE, dtype=torch.float), y_train.to(DEVICE, dtype=torch.float)\n",
    "\n",
    "        output = model(x_train)\n",
    "        loss = criterion(output, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            x_test, y_test = batch\n",
    "            x_test, y_test = x_test.to(DEVICE, dtype=torch.float), y_test.to(DEVICE, dtype=torch.float)\n",
    "\n",
    "            output = model(x_test)\n",
    "            test_loss += criterion(output, y_test)\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            y_test = y_test.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(y_test.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "        \n",
    "    # print the result\n",
    "    print(\"Loss: {:.4f}, Accuracy: {}%\".format(test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training the model...\")\n",
    "for epoch in tqdm(range(1, EPOCHS + 1), desc='EPOCHS'):\n",
    "    train(model, train_loader)\n",
    "    \n",
    "print(\"TRAIN IS SUCCESSFULLY DONE.\")\n",
    "evaluate(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPSquv/dJBnGjSNsrTFjO/u",
   "include_colab_link": true,
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
